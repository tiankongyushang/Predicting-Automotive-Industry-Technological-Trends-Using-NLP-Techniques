{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from operator import methodcaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SECCrawler(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        os.chdir(\".\")\n",
    "        print(\"Download directory will be created here: \",os.getcwd())\n",
    "\n",
    "    def make_directory(self, company_code, cik, priorto, filing_type):\n",
    "        current_directory = os.getcwd()\n",
    "        path = os.path.join(current_directory, company_code)\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "            except OSError as exception:\n",
    "                if exception.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "    def save_in_directory(self, company_code, cik, priorto, doc_list, \n",
    "        doc_name_list, filing_type):\n",
    "        for j in range(len(doc_list)):\n",
    "            base_url = doc_list[j]\n",
    "            r = requests.get(base_url)\n",
    "            data = r.text\n",
    "            current_directory = os.getcwd()\n",
    "            path = os.path.join(current_directory, company_code, \n",
    "                doc_name_list[j])\n",
    "            with open(path, \"ab\") as f:\n",
    "                f.write(data.encode('ascii', 'ignore'))\n",
    "\n",
    "    def filing_10K(self, company_code, cik, priorto, count):\n",
    "        self.make_directory(company_code, cik, priorto, '10-K')\n",
    "        base_url = \"http://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=\"+str(cik)+\\\n",
    "        \"&type=10-K&dateb=\"+str(priorto)+\"&owner=exclude&output=xml&count=\"+str(count)\n",
    "        print (\"started 10-K \" + str(company_code))\n",
    "\n",
    "        r = requests.get(base_url)\n",
    "        data = r.text\n",
    "\n",
    "        # get doc list data\n",
    "        doc_list, doc_name_list = self.create_document_list(data)\n",
    "\n",
    "        try:\n",
    "            self.save_in_directory(company_code, cik, priorto, doc_list, doc_name_list, '10-K')\n",
    "        except Exception as e:\n",
    "            print (str(e))\n",
    "\n",
    "        print (\"Successfully downloaded all the files\")\n",
    "\n",
    "\n",
    "    def create_document_list(self, data):\n",
    "        # parse fetched data using beatifulsoup\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        # store the link in the list\n",
    "        link_list = list()\n",
    "\n",
    "        # If the link is .htm convert it to .html\n",
    "        for link in soup.find_all('filinghref'):\n",
    "            url = link.string\n",
    "            if link.string.split(\".\")[len(link.string.split(\".\"))-1] == \"htm\":\n",
    "                url += \"l\"\n",
    "            link_list.append(url)\n",
    "        link_list_final = link_list\n",
    "\n",
    "        print (\"Number of files to download {0}\".format(len(link_list_final)))\n",
    "        print (\"Starting download....\")\n",
    "\n",
    "        # List of url to the text documents\n",
    "        doc_list = list()\n",
    "        # List of document names\n",
    "        doc_name_list = list()\n",
    "\n",
    "        # Get all the doc\n",
    "        for k in range(len(link_list_final)):\n",
    "            required_url = link_list_final[k].replace('-index.html', '')\n",
    "            txtdoc = required_url + \".txt\"\n",
    "            docname = txtdoc.split(\"/\")[-1]\n",
    "            doc_list.append(txtdoc)\n",
    "            doc_name_list.append(docname)\n",
    "        return doc_list, doc_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies():\n",
    "    with open('companylist.txt', 'r') as file:\n",
    "        clist = file.read().split('\\n')\n",
    "        clist = map(methodcaller(\"split\",\" \"), clist)\n",
    "        return clist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_10_K():\n",
    "    clist = get_companies()\n",
    "    s = SECCrawler()\n",
    "    priorto = '20200101'\n",
    "    count = '5'\n",
    "    for company in clist:\n",
    "        print(\"Downloading for\", company[0])\n",
    "        s.filing_10K(company[0], company[1], priorto, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory will be created here:  C:\\Users\\Lenovo\\Desktop\\Anaconda Python 3\\Text Analytics\\Project\n",
      "Downloading for TSLA\n",
      "started 10-K TSLA\n",
      "Number of files to download 10\n",
      "Starting download....\n",
      "Successfully downloaded all the files\n",
      "Downloading for VSLR\n",
      "started 10-K VSLR\n",
      "Number of files to download 5\n",
      "Starting download....\n",
      "Successfully downloaded all the files\n",
      "Downloading for GE\n",
      "started 10-K GE\n",
      "Number of files to download 10\n",
      "Starting download....\n",
      "Successfully downloaded all the files\n",
      "Downloading for BEP\n",
      "started 10-K BEP\n",
      "Number of files to download 0\n",
      "Starting download....\n",
      "Successfully downloaded all the files\n",
      "Downloading for PEGI\n",
      "started 10-K PEGI\n",
      "Number of files to download 8\n",
      "Starting download....\n",
      "Successfully downloaded all the files\n"
     ]
    }
   ],
   "source": [
    "download_all_10_K()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
